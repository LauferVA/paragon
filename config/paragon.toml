# =============================================================================
# PARAGON CONFIGURATION
# =============================================================================
# This file contains all runtime configuration for the Paragon platform.
# All thresholds, paths, and limits are defined here - no hardcoding in code.
#
# Schema: msgspec-compatible (parsed via tomllib + msgspec validation)
# =============================================================================

[system]
# Logging configuration
log_level = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Parallelism settings (for Granian workers and Polars operations)
max_workers = 4

# Workspace directory for session outputs
workspace_path = "./workspace"

# =============================================================================
# GRAPH DATABASE CONFIGURATION
# =============================================================================

[graph]
# Backend engine (only rustworkx supported in v2.1)
backend = "rustworkx"

# Checkpoint frequency (steps between state persistence)
checkpoint_interval = 50

# Persistence storage path (Polars-friendly format)
storage_path = "./data/graph_state.parquet"

# Maximum nodes before warning (memory management)
max_nodes_warning = 100000

# Wave computation settings
wave_batch_size = 1000  # Max nodes per wave for parallel execution

# =============================================================================
# AGENT CONFIGURATION
# =============================================================================

[agents]
# Default timeout for agent operations (seconds)
timeout_seconds = 30

# Maximum recursion depth for agent reasoning loops
max_recursion_limit = 25

# Default cost limit per agent invocation (USD)
default_cost_limit = 1.00

# Default retry attempts before escalation
default_max_attempts = 3

# Rate limiting (Anthropic API ~50 RPM)
rate_limit_rpm = 50
rate_limit_burst = 10

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

[llm]
# Default provider (anthropic, openai, local, manual)
default_provider = "anthropic"

# Model tier assignments
# Note: Use LiteLLM format "provider/model" for explicit routing
[llm.models]
# High Reasoning tier: Best for complex tasks
heavy = "claude-sonnet-4-5-20250929"
# Standard tier: Fast and capable
standard = "claude-haiku-4-5-20251001"
# Light tier: Quick tasks
light = "claude-haiku-4-5-20251001"

# Legacy model options (still available)
[llm.models.legacy]
sonnet_4 = "claude-sonnet-4-20250514"
haiku_35 = "claude-3-5-haiku-20241022"

# Additional model options (for manual override)
[llm.models.alternatives]
# OpenAI fallback for one-shot coding tasks
openai_codex = "gpt-4o"
# Google flagship
google_pro = "gemini-3-pro-preview"
# Google stable
google_stable = "gemini-2.5-pro"
# OpenAI mini for mundane tasks
openai_mini = "gpt-4o-mini"

# =============================================================================
# ALIGNMENT ENGINE CONFIGURATION
# =============================================================================

[alignment]
# pygmtools algorithm selection
algorithm = "rrwm"  # Reweighted Random Walk Matching

# Minimum alignment score to consider a match
min_score_threshold = 0.7

# =============================================================================
# CODE PARSER CONFIGURATION
# =============================================================================

[parser]
# Tree-sitter configuration
default_language = "python"

# Supported languages (must have tree-sitter grammar installed)
supported_languages = ["python", "javascript", "typescript", "rust", "go"]

# =============================================================================
# API SERVER CONFIGURATION
# =============================================================================

[server]
# Granian server settings
host = "0.0.0.0"
port = 8000

# Threading mode for Granian
# "runtime" uses Rust async runtime (recommended for graph operations)
threading_mode = "runtime"

# CORS settings
cors_origins = ["*"]

# =============================================================================
# TELEMETRY CONFIGURATION
# =============================================================================

[telemetry]
# Enable/disable telemetry
enabled = true

# Telemetry backend (console, file, otlp)
backend = "console"

# Metrics collection interval (seconds)
metrics_interval = 10

# =============================================================================
# OBSERVABILITY CONFIGURATION (Time Machine)
# =============================================================================

[observability]
# Rerun session recording directory
log_dir = "./data/sessions"

# Session file naming pattern (supports: {timestamp}, {session_id})
session_pattern = "{timestamp}_{session_id}.rrd"

# Enable/disable Rerun logging
rerun_enabled = true

# =============================================================================
# RESOURCE GUARD CONFIGURATION (Optimizer)
# =============================================================================

[system.resources]
# RAM threshold (percentage) - pause orchestrator if exceeded
ram_threshold_percent = 90

# CPU threshold (percentage) - pause if sustained for duration_seconds
cpu_threshold_percent = 95

# Duration (seconds) before triggering pause
sustained_duration_seconds = 60

# Poll interval for resource monitoring
poll_interval_seconds = 5

# =============================================================================
# LLM ROUTING CONFIGURATION (Cost Arbitrage)
# =============================================================================
# Routes tasks to appropriate models based on complexity

[llm.routing]
# High-reasoning tasks (Architect, Builder, Coder, Auditor)
# Primary: claude-sonnet-4-5 - Current flagship
high_reasoning_provider = "anthropic"
high_reasoning_model = "claude-sonnet-4-5-20250929"

# Fast reasoner alternative
high_reasoning_fast_provider = "anthropic"
high_reasoning_fast_model = "claude-haiku-4-5-20251001"

# OpenAI fallback for one-shot coding
high_reasoning_openai_provider = "openai"
high_reasoning_openai_model = "gpt-4o"

# Google fallback
high_reasoning_google_provider = "gemini"
high_reasoning_google_model = "gemini-3-pro-preview"

# Mundane tasks (Documenter, LogScrubber, Formatter)
# Primary: claude-haiku-4-5 - Fast and cheap
mundane_provider = "anthropic"
mundane_model = "claude-haiku-4-5-20251001"

# Local fallback (for cost savings or offline use)
mundane_local_provider = "ollama"
mundane_local_model = "llama3.3"

# OpenAI mini fallback
mundane_openai_provider = "openai"
mundane_openai_model = "gpt-4o-mini"

# Fallback if local model unavailable
mundane_fallback_provider = "anthropic"
mundane_fallback_model = "claude-3-5-haiku-20241022"

# Sensitive tasks (PII, Secrets, Internal Logs)
# HARD ROUTE: Local only - network requests blocked for this tier
sensitive_provider = "ollama"
sensitive_model = "llama3.3"

# =============================================================================
# RESEARCH CONFIGURATION (Web Search)
# =============================================================================

[research]
# Search provider (tavily recommended for production)
provider = "tavily"

# API key read from environment variable TAVILY_API_KEY
# (do not put actual key here - use env var)

# Maximum results per search
max_results = 5

# Search depth (basic or advanced)
search_depth = "advanced"

# =============================================================================
# GIT SYNC CONFIGURATION (Historian)
# =============================================================================

[git]
# Enable/disable automatic git commits
enabled = true

# Repository path (default: current directory)
repo_path = "."

# Auto-commit on transaction boundaries
auto_commit = true

# Auto-push after commit (DANGEROUS - use with caution!)
auto_push = false

# Commit message prefix (optional)
commit_prefix = ""

# Git author configuration
author_name = "Paragon"
author_email = "paragon@localhost"

# =============================================================================
# DOCUMENTER CONFIGURATION (Historian)
# =============================================================================

[documenter]
# README path
readme_path = "README.md"

# Changelog path
changelog_path = "CHANGELOG.md"

# Wiki directory
wiki_path = "docs/wiki"

# Auto-generate documentation
auto_generate = true

# Include pending nodes in documentation
include_pending_nodes = false
